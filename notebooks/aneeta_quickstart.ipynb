{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a008887",
   "metadata": {},
   "source": [
    "# ANEETA Evaluation: Local RAG and Model Comparisons\n",
    "This notebook provides a reproducible baseline evaluation of the ANEETA Doubt Solver using local models via Ollama and the existing ANEETA vector stores. It also verifies repository setup and supports simple model comparisons for group reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53d87055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\notebooks\n",
      "Expecting ANEETA at: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\n",
      "Expecting 469ProjectCombined at: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\notebooks\n",
      "Found: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\notebooks\n",
      "Found: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\n",
      "Repos verified.\n"
     ]
    }
   ],
   "source": [
    "# 1) Verify Local Repos and Paths\n",
    "import os, sys, subprocess, shutil\n",
    "from pathlib import Path\n",
    "ROOT = Path.cwd().resolve()\n",
    "aneeta_dir = ROOT.parent / \"ANEETA\"  # sibling repo at same level\n",
    "combo_dir = ROOT  # this notebook lives in 469ProjectCombined\n",
    "print(\"Current working dir:\", ROOT)\n",
    "print(\"Expecting ANEETA at:\", aneeta_dir)\n",
    "print(\"Expecting 469ProjectCombined at:\", combo_dir)\n",
    "\n",
    "# Default ANEETA repository URL (override with env ANEETA_GIT_URL if needed)\n",
    "ANEETA_GIT_URL = os.getenv(\"ANEETA_GIT_URL\", \"https://github.com/BenjaminLohDW/ANEETA.git\")\n",
    "\n",
    "def ensure_repo(path: Path, git_url: str | None = None):\n",
    "    if path.exists():\n",
    "        print(f\"Found: {path}\")\n",
    "        return\n",
    "    if git_url is None:\n",
    "        raise SystemExit(f\"Missing {path}. Please clone it or provide git_url.\")\n",
    "    print(f\"Cloning {git_url} into {path} ...\")\n",
    "    subprocess.run([\"git\", \"clone\", git_url, str(path)], check=True)\n",
    "\n",
    "ensure_repo(combo_dir)\n",
    "ensure_repo(aneeta_dir, ANEETA_GIT_URL)\n",
    "print(\"Repos verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f20bb8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n",
      "Executable: c:\\Users\\yanji\\anaconda3\\envs\\is217_env\\python.exe\n",
      "Platform: Windows-10-10.0.26100-SP0\n",
      "Suggested venv path: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\notebooks\\.venv\n",
      "If you need one, run in PowerShell:\n",
      "python -m venv .venv; .\\.venv\\Scripts\\Activate.ps1; python -m pip install -U pip\n",
      "Torch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# 2) Create and Validate Python Environment\n",
    "import platform, sys, subprocess, os\n",
    "from pathlib import Path\n",
    "print(\"Python:\", sys.version)\n",
    "assert sys.version_info >= (3,10), \"Python >= 3.10 required\"\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optionally create venv instructions (not auto-creating to avoid permission issues)\n",
    "venv_dir = ROOT / \".venv\"\n",
    "print(\"Suggested venv path:\", venv_dir)\n",
    "print(\"If you need one, run in PowerShell:\")\n",
    "print(\"python -m venv .venv; .\\\\.venv\\\\Scripts\\\\Activate.ps1; python -m pip install -U pip\")\n",
    "\n",
    "# CUDA check (optional)\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed or CUDA not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a685551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing core packages (binary wheels preferred)...\n",
      "Core imports OK (lean setup)\n",
      "Core imports OK (lean setup)\n"
     ]
    }
   ],
   "source": [
    "# 3) Install and Import Dependencies (lean and fast on Windows)\n",
    "import sys, subprocess, os\n",
    "\n",
    "# Speed: upgrade pip tooling first\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "\n",
    "# Minimal core deps; avoid heavy extras here\n",
    "# Use binary wheels where possible to speed up installs\n",
    "packages = [\n",
    "    \"typing_extensions>=4.12.2\",\n",
    "    \"pydantic<2.0.0\",        # compatible with chromadb 0.4.x\n",
    "    \"numpy==1.26.4\",         # stable wheels\n",
    "    \"pandas==2.2.2\",         # pin to a wheel-friendly version\n",
    "    \"requests\",\n",
    "    \"tqdm>=4.65.0\",\n",
    "    \"jsonlines\",\n",
    "    \"chromadb==0.4.24\",      # no ONNX default embedder\n",
    "    # Minimal ANEETA runtime deps (pure-Python)\n",
    "    \"langchain\", \"langchain-community\", \"langchain-ollama\", \"langgraph\"\n",
    "]\n",
    "\n",
    "print(\"Installing core packages (binary wheels preferred)...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--only-binary\", \":all:\", *packages], check=False)\n",
    "\n",
    "# Imports (defer MLflow to services cell and make it optional)\n",
    "import chromadb, pandas as pd, numpy as np, requests  # noqa: F401\n",
    "print(\"Core imports OK (lean setup)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96272050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: Provider(kind='ollama', model='llama3.1:8b')\n"
     ]
    }
   ],
   "source": [
    "# 4) Configure Providers (Ollama first; optional OpenAI)\n",
    "import os, json, requests\n",
    "from dataclasses import dataclass\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "@dataclass\n",
    "class Provider:\n",
    "    kind: str  # 'ollama' or 'openai'\n",
    "    model: str\n",
    "\n",
    "def get_available_models():\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        if r.ok:\n",
    "            return {m.get('name') for m in r.json().get('models', []) if 'name' in m}\n",
    "    except Exception:\n",
    "        pass\n",
    "    return set()\n",
    "\n",
    "def pick_provider(prefer_ollama: bool=True) -> Provider:\n",
    "    if prefer_ollama:\n",
    "        installed = get_available_models()\n",
    "        # Prefer a tools-capable local model by default\n",
    "        order = [os.getenv('LLM_MODEL'), 'llama3.1:8b', 'phi3.5', 'gemma2:2b', 'deepseek-r1:7b']\n",
    "        for m in order:\n",
    "            if m and ((not installed) or (m in installed)):\n",
    "                return Provider('ollama', m)\n",
    "    if OPENAI_API_KEY:\n",
    "        return Provider('openai', os.getenv('OPENAI_MODEL', 'gpt-4o-mini'))\n",
    "    # Fallback\n",
    "    return Provider('ollama', 'llama3.1:8b')\n",
    "\n",
    "provider = pick_provider(True)\n",
    "print(\"Using provider:\", provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "207e684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama reachable at http://localhost:11434\n",
      "Pull these tags for the evaluations (pick any/all):\n",
      " - ollama pull llama3.1:8b   # tools-capable local chat model (baseline)\n",
      " - ollama pull gemma2:9b     # higher quality candidate (more RAM)\n",
      " - ollama pull mistral-nemo:12b  # higher quality candidate (heavier)\n",
      " - ollama pull nomic-embed-text  # embeddings needed for vector DB\n",
      "MLflow tracking URI: http://127.0.0.1:5000\n",
      "Reusing existing ChromaDB client. Persisting at: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\notebooks\\.chroma\n",
      "ChromaDB OK.\n"
     ]
    }
   ],
   "source": [
    "# 5) Start Services: Ollama, optional MLflow, ChromaDB\n",
    "import time, requests, os, subprocess, sys, pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Silence Chroma telemetry noise (set before importing chromadb)\n",
    "os.environ.setdefault(\"ANONYMIZED_TELEMETRY\", \"false\")\n",
    "os.environ.setdefault(\"CHROMA_TELEMETRY_ENABLED\", \"false\")\n",
    "\n",
    "\n",
    "def check_ollama(url=OLLAMA_URL):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        return r.status_code in (200, 404)  # 404 is OK for root\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "if not check_ollama():\n",
    "    print(\"Ollama not reachable at\", OLLAMA_URL)\n",
    "    print(\"- Install/start Ollama, then run: ollama serve\")\n",
    "else:\n",
    "    print(\"Ollama reachable at\", OLLAMA_URL)\n",
    "    print(\"Pull these tags for the evaluations (pick any/all):\")\n",
    "    print(\" - ollama pull llama3.1:8b   # tools-capable local chat model (baseline)\")\n",
    "    print(\" - ollama pull gemma2:9b     # higher quality candidate (more RAM)\")\n",
    "    print(\" - ollama pull mistral-nemo:12b  # higher quality candidate (heavier)\")\n",
    "    print(\" - ollama pull nomic-embed-text  # embeddings needed for vector DB\")\n",
    "\n",
    "# Optional MLflow setup (skip if not installed)\n",
    "try:\n",
    "    import mlflow  # noqa: F401\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://127.0.0.1:5000\"))\n",
    "    print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
    "    MLFLOW_OK = True\n",
    "except Exception as e:\n",
    "    print(\"MLflow not installed/available (optional):\", e)\n",
    "    MLFLOW_OK = False\n",
    "\n",
    "# Initialize a persistent Chroma client, reusing any existing kernel client to avoid settings conflicts\n",
    "import chromadb\n",
    "persist_dir = str((ROOT / \".chroma\").resolve())\n",
    "\n",
    "reuse = False\n",
    "if \"client\" in globals() and getattr(globals().get(\"client\"), \"get_or_create_collection\", None):\n",
    "    # Reuse existing client to avoid \"already exists with different settings\" errors\n",
    "    print(\"Reusing existing ChromaDB client. Persisting at:\", persist_dir)\n",
    "    reuse = True\n",
    "\n",
    "if not reuse:\n",
    "    try:\n",
    "        from chromadb.config import Settings\n",
    "        settings = Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "        client = chromadb.Client(settings)\n",
    "        print(\"ChromaDB client created via Settings. Persisting at:\", persist_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Settings-based client failed, falling back to PersistentClient:\", e)\n",
    "        client = chromadb.PersistentClient(path=persist_dir)\n",
    "        print(\"ChromaDB PersistentClient created. Persisting at:\", persist_dir)\n",
    "\n",
    "try:\n",
    "    _ = client.get_or_create_collection(\"smoke_test\")\n",
    "    print(\"ChromaDB OK.\")\n",
    "except ValueError as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(\"Chroma singleton already initialized with different settings; reusing existing client.\")\n",
    "    else:\n",
    "        print(\"ChromaDB init failed:\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea656ad7",
   "metadata": {},
   "source": [
    "### Troubleshooting Ollama model pulls (Windows)\n",
    "If `ollama pull` fails with `pull model manifest: file does not exist`:\n",
    "\n",
    "1) Confirm the service is running\n",
    "- In a new PowerShell: `ollama serve` (if you see `bind: Only one usage...`, it means Ollama is already running; skip this.)\n",
    "- Check: `Invoke-WebRequest http://localhost:11434/ -UseBasicParsing`\n",
    "\n",
    "2) Use valid tags for your version\n",
    "- Try: `ollama pull llama3.1:8b`  \n",
    "- Or: `ollama pull gemma2:2b`\n",
    "- Or: `ollama pull mistral-nemo:12b`\n",
    "\n",
    "3) Update Ollama\n",
    "- Download the latest from https://ollama.com/download and reinstall.\n",
    "\n",
    "4) Clear local cache (optional)\n",
    "- Stop service; delete `%LOCALAPPDATA%\\Ollama\\models` cautiously; then `ollama serve` and pull again.\n",
    "\n",
    "Note: For ANEETA Doubt Solver, you DO need the embedding model `nomic-embed-text` to open the existing Chroma vector stores. Pull it with: `ollama pull nomic-embed-text`. Also prefer a tools-capable chat model like `llama3.1:8b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4439633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ANEETA repo processed data at: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\\Processed Data\n",
      "Loaded rows: 8890 from: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\\Processed Data\\solved_question_papers.json\n",
      "Loaded rows: 8890 from: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\\Processed Data\\solved_question_papers.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neet:1:0</td>\n",
       "      <td>Two objects of mass 10 kg and 20 kg respective...</td>\n",
       "      <td></td>\n",
       "      <td>(3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neet:2:1</td>\n",
       "      <td>Match List-I with List-II \\n List-I \\n(Electro...</td>\n",
       "      <td></td>\n",
       "      <td>(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neet:3:2</td>\n",
       "      <td>The energy that will be ideally radiated by a ...</td>\n",
       "      <td></td>\n",
       "      <td>(2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           question context answer\n",
       "0  neet:1:0  Two objects of mass 10 kg and 20 kg respective...            (3)\n",
       "1  neet:2:1  Match List-I with List-II \\n List-I \\n(Electro...            (1)\n",
       "2  neet:3:2  The energy that will be ideally radiated by a ...            (2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Load ANEETA Dataset → normalize to DataFrame (repo data only)\n",
    "import pandas as pd, json, os, re\n",
    "from pathlib import Path\n",
    "\n",
    "# Local convenience path (optional)\n",
    "local_processed = (ROOT / \"datasets\" / \"aneeta\" / \"processed\").resolve()\n",
    "# Authoritative repo path\n",
    "repo_processed = (aneeta_dir / \"Processed Data\").resolve()\n",
    "\n",
    "rows = []\n",
    "source_used = None\n",
    "\n",
    "# 1) Prefer local test.jsonl if present (explicit test set)\n",
    "test_path = local_processed / \"test.jsonl\"\n",
    "if test_path.exists():\n",
    "    import jsonlines\n",
    "    with jsonlines.open(test_path, 'r') as reader:\n",
    "        rows = list(reader)\n",
    "    source_used = str(test_path)\n",
    "\n",
    "# Helper: specialized parser for NEET solved papers (extract Q and numeric answer)\n",
    "def parse_solved_mcqs(json_path: Path):\n",
    "    try:\n",
    "        data = json.loads(json_path.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        return []\n",
    "    # Concatenate pages and search for patterns like: \"1. <question/body> ... Answer (3)\"\n",
    "    text = \"\\n\\n\".join([str(obj.get(\"page_content\", \"\")) for obj in data])\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    # Regex: capture question number, question block (non-greedy), and the numeric answer in parentheses\n",
    "    pat = re.compile(r\"(?:^|\\n)\\s*(\\d{1,3})\\.\\s*(.*?)(?:\\n\\s*Answer\\s*\\((\\d+)\\))\", re.S)\n",
    "    out = []\n",
    "    used_ids = set()\n",
    "    for i, m in enumerate(pat.finditer(text)):\n",
    "        qn = m.group(1) or str(i)\n",
    "        body = (m.group(2) or \"\").strip()\n",
    "        # Truncate at a solution marker if present to keep the stem/options only\n",
    "        body = body.split(\"\\nSol.\")[0].strip()\n",
    "        # Normalize whitespace\n",
    "        body = re.sub(r\"[ \\t]+\", \" \", body)\n",
    "        ans_num = m.group(3)\n",
    "        # Build a stable id even if question numbers repeat across sections\n",
    "        uid = f\"neet:{qn}:{i}\"\n",
    "        if uid in used_ids:\n",
    "            continue\n",
    "        used_ids.add(uid)\n",
    "        # Keep the whole body as the question (includes options like (1)...(4) when present)\n",
    "        out.append({\n",
    "            \"id\": uid,\n",
    "            \"question\": body,\n",
    "            \"context\": \"\",\n",
    "            \"answer\": f\"({ans_num})\"\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# 2) Otherwise, strictly load from ANEETA repo's Processed Data\n",
    "if not rows:\n",
    "    if not repo_processed.exists():\n",
    "        raise SystemExit(f\"ANEETA processed data folder not found at: {repo_processed}\")\n",
    "    print(\"Using ANEETA repo processed data at:\", repo_processed)\n",
    "\n",
    "    # First, try specialized parse from solved_question_papers.json for MCQ Q/A pairs\n",
    "    solved_path = repo_processed / \"solved_question_papers.json\"\n",
    "    if solved_path.exists():\n",
    "        parsed = parse_solved_mcqs(solved_path)\n",
    "        if parsed:\n",
    "            rows = parsed\n",
    "            source_used = str(solved_path)\n",
    "\n",
    "    # If still empty, fall back to generic extraction from other processed files (context only)\n",
    "    if not rows:\n",
    "        candidates = [\n",
    "            \"mentor_data.json\",\n",
    "            \"processed_biology_chunks.json\",\n",
    "            \"processed_chemistry_chunks.json\",\n",
    "            \"processed_physics_chunks.json\",\n",
    "        ]\n",
    "        def load_json(path: Path):\n",
    "            try:\n",
    "                data = json.loads(path.read_text(encoding='utf-8'))\n",
    "                if isinstance(data, dict):\n",
    "                    # flatten dict-of-lists if needed\n",
    "                    flat = []\n",
    "                    for v in data.values():\n",
    "                        if isinstance(v, list):\n",
    "                            flat.extend(v)\n",
    "                    return flat or [data]\n",
    "                return data if isinstance(data, list) else [data]\n",
    "            except Exception:\n",
    "                return []\n",
    "        def get_ci(d: dict, keys: list[str]):\n",
    "            # case-insensitive get\n",
    "            lk = {k.lower(): k for k in d.keys()}\n",
    "            for k in keys:\n",
    "                if k.lower() in lk:\n",
    "                    return d.get(lk[k.lower()])\n",
    "            return None\n",
    "        extracted = []\n",
    "        used_file = None\n",
    "        for name in candidates:\n",
    "            f = repo_processed / name\n",
    "            if not f.exists():\n",
    "                continue\n",
    "            data = load_json(f)\n",
    "            for i, rec in enumerate(data):\n",
    "                if not isinstance(rec, dict):\n",
    "                    continue\n",
    "                # These processed files generally have 'page_content' text chunks\n",
    "                ctx = get_ci(rec, [\"page_content\",\"content\",\"text\",\"chunk\"]) or \"\"\n",
    "                if isinstance(ctx, (dict, list)):\n",
    "                    ctx = json.dumps(ctx, ensure_ascii=False)\n",
    "                if isinstance(ctx, str) and len(ctx.strip()) >= 20:\n",
    "                    # Use the chunk as context and prompt the agent with a generic instruction\n",
    "                    q_text = ctx.strip().split(\"\\n\")[0]\n",
    "                    extracted.append({\n",
    "                        \"id\": f\"{name}:{i}\",\n",
    "                        \"question\": q_text[:500],\n",
    "                        \"context\": ctx[:2000],\n",
    "                        \"answer\": \"\"\n",
    "                    })\n",
    "            if extracted:\n",
    "                rows = extracted\n",
    "                used_file = f\n",
    "                break\n",
    "        source_used = str(used_file) if used_file else source_used\n",
    "\n",
    "# Build DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if df.empty:\n",
    "    raise SystemExit(\"Empty dataset after parsing ANEETA data.\")\n",
    "print(\"Loaded rows:\", len(df), \"from:\", source_used)\n",
    "\n",
    "# Normalize columns and order\n",
    "df = df.rename(columns={\"sample_id\":\"id\"})\n",
    "for col in [\"id\",\"question\",\"context\",\"answer\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "# Cap dataset size for quick iteration\n",
    "max_n = int(os.getenv(\"ANEETA_MAX_RECORDS\", \"250\"))\n",
    "df = df[[\"id\",\"question\",\"context\",\"answer\"]].head(max_n)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "763f9d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonlines installed and imported\n"
     ]
    }
   ],
   "source": [
    "# HOTFIX: ensure jsonlines is available\n",
    "import sys, subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"jsonlines\"], check=True)\n",
    "import jsonlines; print(\"jsonlines installed and imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e890a",
   "metadata": {},
   "source": [
    "## Baseline ANEETA Doubt Solver (non‑DSPy)\n",
    "The following cells call ANEETA’s mcq_question_solver_agent through the LangGraph workflow to provide a baseline Doubt Solver run without DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11bfb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_MODEL: llama3.1:8b\n",
      "CREATIVE_LLM_MODEL: llama3.1:8b\n",
      "EMBEDDING_MODEL: nomic-embed-text\n",
      "ANEETA VECTORDB_BASE_PATH: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\\src\\aneeta\\vectordb\n",
      "PYTHONPATH +: C:\\Users\\yanji\\Desktop\\IS469\\Project\\469ProjectCombined\\ANEETA\\src\n",
      "ANEETA resources -> llm.model: llama3.1:8b\n",
      "Smoke test:\n",
      "ANEETA resources -> llm.model: llama3.1:8b\n",
      "Smoke test:\n",
      "Answer:\n",
      "(B)\n",
      "Mit\n",
      "ochond\n",
      "ria\n",
      "Explanation:\n",
      "The\n",
      "mitochond\n",
      "ria\n",
      "are\n",
      "often\n",
      "referred\n",
      "to\n",
      "as\n",
      "the\n",
      "\"\n",
      "power\n",
      "house\n",
      "\"\n",
      "of\n",
      "the\n",
      "cell\n",
      "because\n",
      "they\n",
      "generate\n",
      "most\n",
      "of\n",
      "the\n",
      "cell\n",
      "'s\n",
      "supply\n",
      "of\n",
      "ad\n",
      "enos\n",
      "ine\n",
      "tri\n",
      "ph\n",
      "osphate\n",
      "(\n",
      "AT\n",
      "P\n",
      "),\n",
      "which\n",
      "is\n",
      "used\n",
      "as\n",
      "a\n",
      "source\n",
      "of\n",
      "chemical\n",
      "energy.\n",
      "In\n",
      "addition\n",
      "to\n",
      "generating\n",
      "ATP,\n",
      "mitochond\n",
      "ria\n",
      "are\n",
      "i\n",
      "Answer:\n",
      "(B)\n",
      "Mit\n",
      "ochond\n",
      "ria\n",
      "Explanation:\n",
      "The\n",
      "mitochond\n",
      "ria\n",
      "are\n",
      "often\n",
      "referred\n",
      "to\n",
      "as\n",
      "the\n",
      "\"\n",
      "power\n",
      "house\n",
      "\"\n",
      "of\n",
      "the\n",
      "cell\n",
      "because\n",
      "they\n",
      "generate\n",
      "most\n",
      "of\n",
      "the\n",
      "cell\n",
      "'s\n",
      "supply\n",
      "of\n",
      "ad\n",
      "enos\n",
      "ine\n",
      "tri\n",
      "ph\n",
      "osphate\n",
      "(\n",
      "AT\n",
      "P\n",
      "),\n",
      "which\n",
      "is\n",
      "used\n",
      "as\n",
      "a\n",
      "source\n",
      "of\n",
      "chemical\n",
      "energy.\n",
      "In\n",
      "addition\n",
      "to\n",
      "generating\n",
      "ATP,\n",
      "mitochond\n",
      "ria\n",
      "are\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# Configure ANEETA environment (local-only)\n",
    "import os, sys, re\n",
    "\n",
    "# Ensure env is set BEFORE any aneeta imports\n",
    "non_tools = {\"phi3.5\", \"gemma2:2b\", \"deepseek-r1:7b\"}\n",
    "if os.getenv(\"LLM_MODEL\") in non_tools or not os.getenv(\"LLM_MODEL\"):\n",
    "    os.environ[\"LLM_MODEL\"] = \"llama3.1:8b\"\n",
    "if os.getenv(\"CREATIVE_LLM_MODEL\") in non_tools or not os.getenv(\"CREATIVE_LLM_MODEL\"):\n",
    "    os.environ[\"CREATIVE_LLM_MODEL\"] = \"llama3.1:8b\"\n",
    "# Embeddings are required to open the persisted Chroma vector stores; use the model that matches the DBs\n",
    "os.environ.setdefault(\"EMBEDDING_MODEL\", \"nomic-embed-text\")\n",
    "# Use the ANEETA repo's persisted Chroma DBs\n",
    "os.environ.setdefault(\"VECTORDB_BASE_PATH\", str((aneeta_dir / \"src\" / \"aneeta\" / \"vectordb\").resolve()))\n",
    "\n",
    "print(\"LLM_MODEL:\", os.environ[\"LLM_MODEL\"]) \n",
    "print(\"CREATIVE_LLM_MODEL:\", os.environ[\"CREATIVE_LLM_MODEL\"]) \n",
    "print(\"EMBEDDING_MODEL:\", os.environ[\"EMBEDDING_MODEL\"])  # tip: if missing locally, run: ollama pull nomic-embed-text\n",
    "print(\"ANEETA VECTORDB_BASE_PATH:\", os.environ[\"VECTORDB_BASE_PATH\"])\n",
    "\n",
    "# Add ANEETA src to path for imports\n",
    "sys.path.insert(0, str((aneeta_dir / \"src\").resolve()))\n",
    "print(\"PYTHONPATH +:\", (aneeta_dir/\"src\").resolve())\n",
    "\n",
    "# Hard reset: clear Streamlit cache and fully purge ANEETA modules so model changes take effect\n",
    "import importlib\n",
    "try:\n",
    "    import streamlit as st\n",
    "    try:\n",
    "        st.cache_resource.clear()\n",
    "        print(\"Cleared streamlit cache_resource\")\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    st = None  # type: ignore\n",
    "\n",
    "# Purge ALL aneeta modules to avoid stale singletons (llm, resources, workflow, etc.)\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod == \"aneeta\" or mod.startswith(\"aneeta.\"):\n",
    "        sys.modules.pop(mod, None)\n",
    "\n",
    "# Import in a clean order: LLM -> resources -> agents/router -> workflow\n",
    "import aneeta.llm.llm as llm_mod\n",
    "importlib.reload(llm_mod)\n",
    "\n",
    "import aneeta.core.resources as resources_mod\n",
    "importlib.reload(resources_mod)\n",
    "\n",
    "import aneeta.nodes.agents as agents_mod\n",
    "import aneeta.nodes.router as router_mod\n",
    "importlib.reload(agents_mod)\n",
    "importlib.reload(router_mod)\n",
    "\n",
    "import aneeta.graph.workflow as workflow_mod\n",
    "importlib.reload(workflow_mod)\n",
    "\n",
    "# Print resolved model from ANEETA resources for verification\n",
    "try:\n",
    "    from aneeta.core.resources import llm as current_llm\n",
    "    resolved_model = getattr(current_llm, \"model\", None) or getattr(current_llm, \"model_name\", None) or getattr(current_llm, \"model_id\", None)\n",
    "    print(\"ANEETA resources -> llm.model:\", resolved_model)\n",
    "except Exception as e:\n",
    "    print(\"Could not read ANEETA resources llm model:\", e)\n",
    "\n",
    "# Define a simple whitespace normalizer to clean streamed outputs\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    t = text.replace(\"\\r\", \"\")\n",
    "    # Trim spaces around newlines\n",
    "    t = re.sub(r\"[ \\t]+\\n\", \"\\n\", t)\n",
    "    t = re.sub(r\"\\n[ \\t]+\", \"\\n\", t)\n",
    "    # Collapse 3+ newlines to 2\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    # Collapse runs of spaces\n",
    "    t = re.sub(r\" {2,}\", \" \", t)\n",
    "    # Remove space before punctuation like , . ; : ! ?\n",
    "    t = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", t)\n",
    "    # Fix spaced MCQ options like ( A ) -> (A)\n",
    "    t = re.sub(r\"\\(\\s*([A-D1-4])\\s*\\)\", r\"(\\1)\", t)\n",
    "    return t.strip()\n",
    "\n",
    "# Define baseline solve_mcq using ANEETA LangGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def solve_mcq(question: str, timeout: int = 120, language: str | None = None):\n",
    "    try:\n",
    "        graph = workflow_mod.get_graph()\n",
    "        init_state = {\n",
    "            \"messages\": [HumanMessage(content=question)],\n",
    "            \"user_explanation_language\": language or os.getenv(\"ANEETA_EXPLANATION_LANG\", \"English\"),\n",
    "        }\n",
    "        # Stream and collect final text (supports either messages or response_stream)\n",
    "        final_text = []\n",
    "        for update in graph.stream(init_state):\n",
    "            for _, v in update.items():\n",
    "                if isinstance(v, dict):\n",
    "                    # If agent provided a streaming generator, consume it\n",
    "                    rs = v.get(\"response_stream\")\n",
    "                    if rs is not None:\n",
    "                        try:\n",
    "                            for chunk in rs:\n",
    "                                t = str(getattr(chunk, \"content\", chunk)).strip()\n",
    "                                if t:\n",
    "                                    final_text.append(t)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    # Also collect from messages if present\n",
    "                    if \"messages\" in v:\n",
    "                        msgs = v.get(\"messages\") or []\n",
    "                        if msgs and hasattr(msgs[-1], \"content\"):\n",
    "                            final_text.append(str(msgs[-1].content))\n",
    "        raw_text = \"\\n\".join(final_text).strip()\n",
    "        return {\"text\": normalize_whitespace(raw_text)}\n",
    "    except Exception as e:\n",
    "        return {\"text\": f\"[baseline error: {e}]\"}\n",
    "\n",
    "# Smoke test\n",
    "test_q = \"Which organelle is the powerhouse of the cell? Options: (A) Ribosome (B) Mitochondria (C) Golgi (D) Lysosome\"\n",
    "print(\"Smoke test:\")\n",
    "print(solve_mcq(test_q).get(\"text\", \"\" )[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7095131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANEETA package path: None\n",
      "LLM_MODEL (env): llama3.1:8b\n",
      "LLM (resolved by ANEETA): llama3.1:8b\n",
      "Ollama installed models: ['Student:latest', 'deepseek-r1:7b', 'gemma2:2b', 'gemma2:9b', 'gemma3:1b', 'llama3.1:8b', 'mistral-nemo:12b', 'nomic-embed-text:latest'] ...+2 more\n",
      "LLM (resolved by ANEETA): llama3.1:8b\n",
      "Ollama installed models: ['Student:latest', 'deepseek-r1:7b', 'gemma2:2b', 'gemma2:9b', 'gemma3:1b', 'llama3.1:8b', 'mistral-nemo:12b', 'nomic-embed-text:latest'] ...+2 more\n",
      "LLM connection OK.\n",
      "LLM connection OK.\n"
     ]
    }
   ],
   "source": [
    "# Verify 'aneeta' import source and Ollama model availability\n",
    "import os, importlib, requests\n",
    "import aneeta\n",
    "print(\"ANEETA package path:\", getattr(aneeta, \"__file__\", aneeta))\n",
    "print(\"LLM_MODEL (env):\", os.environ.get(\"LLM_MODEL\"))\n",
    "try:\n",
    "    from aneeta.llm.llm import get_llm\n",
    "    llm = get_llm()\n",
    "    # Many LangChain chat models expose .model_name, .model, or .model_id\n",
    "    actual_model = getattr(llm, \"model\", None) or getattr(llm, \"model_name\", None) or getattr(llm, \"model_id\", None)\n",
    "    print(\"LLM (resolved by ANEETA):\", actual_model)\n",
    "except Exception as e:\n",
    "    print(\"Could not instantiate LLM to inspect model:\", e)\n",
    "\n",
    "try:\n",
    "    r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
    "    if r.ok:\n",
    "        available = {m.get('name') for m in r.json().get('models', []) if 'name' in m}\n",
    "    else:\n",
    "        available = set()\n",
    "    shown = sorted(list(available))[:8]\n",
    "    suffix = f\"...+{max(0, len(available)-8)} more\" if len(available) > 8 else \"\"\n",
    "    print(\"Ollama installed models:\", shown, suffix)\n",
    "    need = os.environ.get(\"LLM_MODEL\")\n",
    "    if need and (available and need not in available):\n",
    "        print(f\"Note: model '{need}' not found in Ollama. Pull it: ollama pull {need}\")\n",
    "except Exception as e:\n",
    "    print(\"Could not query Ollama tags:\", e)\n",
    "\n",
    "# Quick LLM connection smoke (will fail fast if model/server not ready)\n",
    "try:\n",
    "    from aneeta.llm.llm import get_llm\n",
    "    _ = get_llm().invoke(\"hello\")\n",
    "    print(\"LLM connection OK.\")\n",
    "except Exception as e:\n",
    "    print(\"LLM connection check failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da468ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>latency_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>38550.438643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2:9b</td>\n",
       "      <td>47156.552792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-nemo:12b</td>\n",
       "      <td>48115.099192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model    latency_ms\n",
       "0       llama3.1:8b  38550.438643\n",
       "1         gemma2:9b  47156.552792\n",
       "2  mistral-nemo:12b  48115.099192"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick model switch micro-benchmark (local)\n",
    "# This is optional; uses installed models only\n",
    "try:\n",
    "    r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
    "    names = [m.get('name') for m in r.json().get('models', [])]\n",
    "except Exception:\n",
    "    names = []\n",
    "\n",
    "# Shortlist: baseline + two higher-quality candidates\n",
    "model_shortlist = [\"llama3.1:8b\", \"gemma2:9b\", \"mistral-nemo:12b\"]\n",
    "preferred = [m for m in model_shortlist if (not names) or (m in names)]\n",
    "if not preferred:\n",
    "    preferred = [\"llama3.1:8b\"]  # safe default\n",
    "\n",
    "benchmarks = []\n",
    "for name in preferred:\n",
    "    os.environ[\"LLM_MODEL\"] = name\n",
    "    s = time.time(); _ = solve_mcq(\"What is the capital of France? Options: (A) Rome (B) Paris (C) Berlin (D) Madrid\"); dt = (time.time()-s)*1000\n",
    "    benchmarks.append({\"model\": name, \"latency_ms\": dt})\n",
    "\n",
    "pd.DataFrame(benchmarks).sort_values(\"latency_ms\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 3 MCQs across different local models and (optionally) log to MLflow\n",
    "import os, numpy as np, pandas as pd, time, requests\n",
    "\n",
    "# Quick reachability check to avoid confusing errors\n",
    "def _ollama_ok(url: str) -> bool:\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        return r.status_code in (200, 404)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if not _ollama_ok(OLLAMA_URL):\n",
    "    print(\"Ollama not reachable at\", OLLAMA_URL)\n",
    "    print(\"Start the service in a PowerShell:\")\n",
    "    print(\"ollama serve    # then rerun this cell\")\n",
    "else:\n",
    "    # Helper: get available Ollama tags to filter the test set\n",
    "    available = set()\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
    "        if r.ok:\n",
    "            for m in r.json().get('models', []):\n",
    "                if 'name' in m:\n",
    "                    available.add(m['name'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Shortlist for one-click comparisons\n",
    "    model_shortlist = [\"llama3.1:8b\", \"gemma2:9b\", \"mistral-nemo:12b\"]\n",
    "    models_to_test = [m for m in model_shortlist if (not available) or (m in available)]\n",
    "    missing = [m for m in model_shortlist if m not in models_to_test]\n",
    "    if missing:\n",
    "        print(\"Missing models not found in Ollama; pull as needed:\")\n",
    "        for m in missing:\n",
    "            print(f\" - ollama pull {m}\")\n",
    "    if not models_to_test:\n",
    "        models_to_test = [\"llama3.1:8b\"]  # safe default\n",
    "\n",
    "    # Self-contained MCQ examples\n",
    "    mcq_examples = [\n",
    "        \"A body moving in a circle of radius r with speed v has centripetal acceleration? Options: (A) v^2/r (B) r/v^2 (C) v/r^2 (D) r^2/v\",\n",
    "        \"If 2 mol of an ideal gas at constant temperature compress to half the volume, what happens to pressure? Options: (A) doubles (B) halves (C) same (D) zero\",\n",
    "        \"Which organelle is the powerhouse of the cell? Options: (A) Ribosome (B) Mitochondria (C) Golgi (D) Lysosome\",\n",
    "    ]\n",
    "\n",
    "    def eval_model(model_name: str):\n",
    "        os.environ[\"LLM_MODEL\"] = model_name\n",
    "        results, latencies = [], []\n",
    "        for q in mcq_examples:\n",
    "            t0 = time.time()\n",
    "            out = solve_mcq(q)\n",
    "            latencies.append((time.time()-t0)*1000)\n",
    "            results.append(out.get(\"text\",\"\"))\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"latency_p50_ms\": float(np.percentile(latencies, 50)),\n",
    "            \"latency_p95_ms\": float(np.percentile(latencies, 95)),\n",
    "            \"answers\": results,\n",
    "        }\n",
    "\n",
    "    # Optional MLflow logging\n",
    "    try:\n",
    "        import mlflow\n",
    "        mlflow.set_experiment(\"aneeta-baseline-doubtsolver\")\n",
    "        rows = []\n",
    "        for m in models_to_test:\n",
    "            with mlflow.start_run(run_name=f\"baseline_{m}\") as run:\n",
    "                r = eval_model(m)\n",
    "                mlflow.log_params({\"model\": m, \"agent\": \"mcq_question_solver\"})\n",
    "                mlflow.log_metrics({\"latency_p50_ms\": r[\"latency_p50_ms\"], \"latency_p95_ms\": r[\"latency_p95_ms\"]})\n",
    "                rows.append(r)\n",
    "        pd.DataFrame(rows)\n",
    "    except Exception:\n",
    "        # If mlflow isn't installed/available, just run and show a table\n",
    "        rows = [eval_model(m) for m in models_to_test]\n",
    "        pd.DataFrame(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is217_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
