{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Colab Setup for DSPy Evaluation\n",
        "\n",
        "This notebook provides setup instructions and utilities for running DSPy evaluation experiments in Google Colab.\n",
        "\n",
        "## Features\n",
        "- Automatic environment setup\n",
        "- GPU detection and configuration\n",
        "- Model loading and configuration\n",
        "- Evaluation pipeline setup\n",
        "- Results visualization\n",
        "\n",
        "## Owner\n",
        "**Zheng Kai** - Responsible for Colab integration and GPU optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install dspy-ai\n",
        "%pip install openai\n",
        "%pip install together\n",
        "%pip install transformers\n",
        "%pip install torch\n",
        "%pip install datasets\n",
        "%pip install mlflow\n",
        "%pip install fastapi\n",
        "%pip install uvicorn\n",
        "%pip install pydantic\n",
        "%pip install pyyaml\n",
        "%pip install jsonlines\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install tqdm\n",
        "%pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/your-org/dspy-eval.git\n",
        "%cd dspy-eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available. Using CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment setup\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Set environment variables for Colab\n",
        "os.environ['COLAB'] = 'true'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0' if torch.cuda.is_available() else ''\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path: {sys.path[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import project modules\n",
        "try:\n",
        "    from src.core.config import load_config\n",
        "    from src.core.logging import setup_logging\n",
        "    from src.core.artifacts import ArtifactManager\n",
        "    from src.core.telemetry import TelemetryManager\n",
        "    from datasets.registry import get_dataset\n",
        "    print(\"✓ Project modules imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Import error: {e}\")\n",
        "    print(\"Make sure the project structure is correct\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure models for Colab\n",
        "import dspy\n",
        "import openai\n",
        "import together\n",
        "\n",
        "# TODO: Set your API keys\n",
        "# You can also use Colab's secrets manager\n",
        "# from google.colab import userdata\n",
        "# openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "# together.api_key = userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "# For now, use placeholder keys\n",
        "openai.api_key = \"your-openai-key-here\"\n",
        "together.api_key = \"your-together-key-here\"\n",
        "\n",
        "# Configure DSPy LM\n",
        "if torch.cuda.is_available():\n",
        "    # Use GPU-optimized model\n",
        "    lm = dspy.OpenAI(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        api_key=openai.api_key,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.7\n",
        "    )\n",
        "else:\n",
        "    # Use CPU-optimized model\n",
        "    lm = dspy.OpenAI(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        api_key=openai.api_key,\n",
        "        max_tokens=500,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "dspy.settings.configure(lm=lm)\n",
        "print(f\"✓ DSPy configured with {lm.model}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample data\n",
        "try:\n",
        "    # Load SharkTank data\n",
        "    sharktank_data = get_dataset(\"sharktank\", split=\"test\", num_samples=5)\n",
        "    print(f\"✓ Loaded {len(sharktank_data)} SharkTank samples\")\n",
        "    \n",
        "    # Load ANEETA data\n",
        "    aneeta_data = get_dataset(\"aneeta\", split=\"test\", num_samples=5)\n",
        "    print(f\"✓ Loaded {len(aneeta_data)} ANEETA samples\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading data: {e}\")\n",
        "    print(\"Creating sample data...\")\n",
        "    \n",
        "    # Create sample data if loading fails\n",
        "    sharktank_data = [\n",
        "        {\n",
        "            \"sample_id\": \"st_001\",\n",
        "            \"product_facts\": \"AI-powered fitness app\",\n",
        "            \"guidelines\": \"Focus on market opportunity\",\n",
        "            \"pitch\": \"Sample pitch for fitness app\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    aneeta_data = [\n",
        "        {\n",
        "            \"sample_id\": \"an_001\",\n",
        "            \"question\": \"What is machine learning?\",\n",
        "            \"context\": \"ML is a subset of AI\",\n",
        "            \"answer\": \"Machine learning is...\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(\"✓ Created sample data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample data\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "print(\"SharkTank Sample Data:\")\n",
        "print(json.dumps(sharktank_data[0], indent=2))\n",
        "\n",
        "print(\"\\nANEETA Sample Data:\")\n",
        "print(json.dumps(aneeta_data[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a simple evaluation\n",
        "def run_simple_eval(data, project_name):\n",
        "    \"\"\"Run a simple evaluation on the data.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, sample in enumerate(data):\n",
        "        try:\n",
        "            # Simple evaluation logic\n",
        "            if project_name == \"sharktank\":\n",
        "                # Evaluate pitch generation\n",
        "                result = {\n",
        "                    \"sample_id\": sample[\"sample_id\"],\n",
        "                    \"input\": sample[\"product_facts\"],\n",
        "                    \"prediction\": \"Generated pitch placeholder\",\n",
        "                    \"quality_score\": 8.0,\n",
        "                    \"latency_ms\": 1000\n",
        "                }\n",
        "            else:\n",
        "                # Evaluate question answering\n",
        "                result = {\n",
        "                    \"sample_id\": sample[\"sample_id\"],\n",
        "                    \"input\": sample[\"question\"],\n",
        "                    \"prediction\": \"Generated answer placeholder\",\n",
        "                    \"quality_score\": 8.5,\n",
        "                    \"latency_ms\": 800\n",
        "                }\n",
        "            \n",
        "            results.append(result)\n",
        "            print(f\"✓ Processed sample {i+1}/{len(data)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error processing sample {i+1}: {e}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluations\n",
        "print(\"Running SharkTank evaluation...\")\n",
        "sharktank_results = run_simple_eval(sharktank_data, \"sharktank\")\n",
        "\n",
        "print(\"\\nRunning ANEETA evaluation...\")\n",
        "aneeta_results = run_simple_eval(aneeta_data, \"aneeta\")\n",
        "\n",
        "print(f\"\\n✓ Completed evaluations:\")\n",
        "print(f\"  SharkTank: {len(sharktank_results)} samples\")\n",
        "print(f\"  ANEETA: {len(aneeta_results)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create results DataFrame\n",
        "sharktank_df = pd.DataFrame(sharktank_results)\n",
        "aneeta_df = pd.DataFrame(aneeta_results)\n",
        "\n",
        "# Plot quality scores\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# SharkTank quality scores\n",
        "axes[0].hist(sharktank_df['quality_score'], bins=10, alpha=0.7, color='blue')\n",
        "axes[0].set_title('SharkTank Quality Scores')\n",
        "axes[0].set_xlabel('Quality Score')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# ANEETA quality scores\n",
        "axes[1].hist(aneeta_df['quality_score'], bins=10, alpha=0.7, color='green')\n",
        "axes[1].set_title('ANEETA Quality Scores')\n",
        "axes[1].set_xlabel('Quality Score')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(f\"SharkTank - Mean Quality: {sharktank_df['quality_score'].mean():.2f}\")\n",
        "print(f\"SharkTank - Mean Latency: {sharktank_df['latency_ms'].mean():.0f}ms\")\n",
        "print(f\"ANEETA - Mean Quality: {aneeta_df['quality_score'].mean():.2f}\")\n",
        "print(f\"ANEETA - Mean Latency: {aneeta_df['latency_ms'].mean():.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results directory\n",
        "results_dir = Path(\"colab_results\")\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save results with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "with open(results_dir / f\"sharktank_results_{timestamp}.json\", 'w') as f:\n",
        "    json.dump(sharktank_results, f, indent=2)\n",
        "\n",
        "with open(results_dir / f\"aneeta_results_{timestamp}.json\", 'w') as f:\n",
        "    json.dump(aneeta_results, f, indent=2)\n",
        "\n",
        "print(f\"✓ Results saved to {results_dir}\")\n",
        "print(f\"  SharkTank: sharktank_results_{timestamp}.json\")\n",
        "print(f\"  ANEETA: aneeta_results_{timestamp}.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Set up API keys** in Colab secrets\n",
        "2. **Load actual datasets** from the repository\n",
        "3. **Run full evaluations** using the CLI tools\n",
        "4. **Compare results** across different models\n",
        "5. **Export results** for further analysis\n",
        "\n",
        "## GPU Optimization Tips\n",
        "\n",
        "- Use smaller batch sizes for GPU memory efficiency\n",
        "- Enable mixed precision training\n",
        "- Monitor GPU memory usage\n",
        "- Use gradient checkpointing for large models\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- If imports fail, check the project structure\n",
        "- If GPU is not detected, restart runtime\n",
        "- If memory issues occur, reduce batch size\n",
        "- If API errors occur, check your API keys\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
